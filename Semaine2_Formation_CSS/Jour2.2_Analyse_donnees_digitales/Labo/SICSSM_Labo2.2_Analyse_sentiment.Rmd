---
title: 'Labo 2.2: Analyse quantitative du texte'
subtitle: 'Analyse de sentiments'
author: "Visseho Adjiwanou, PhD."
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

# Analyse de sentiments

L'anlyse de sentiment est une technique d'extraction d'information dans un corpus. Le but est de détecter les mots chrechés/voulus dans le corpus. Pour cela, on va se baser sur les dictionnaires qui exsitent ou définir son propre dictionnaire.


```{r}

rm(list = ls())
#install.packages("tidytext")
#install.packages("textdata")

library(tidyverse)
library(tidytext)
library(textdata) # 
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)


```

Nous allons charger les données de trump tweet


```{r}

Senator_data <- read_csv("../Données/Senators_Twitter_Data.csv")

```


```{r}

load(url("https://cbail.github.io/Trump_Tweets.Rdata")) #```{r} I activated this one and commented the following code. The trumptweets <- readRDS("trumptweets.RData") throws error.


# trumptweets <- readRDS("trumptweets.RData")

library(tidytext) #```{r} I added the library function for tidytext and dplyr. Without them the pipe operator was not                          operating 
library(dplyr) 

tidy_trump_tweets <- 
  trumptweets %>% 
  select(created_at, text) %>% 
  unnest_tokens("word", text)     # Tokenise the data
  
tidy_trump_tweets  

head(tidy_trump_tweets, 12)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))

```

```{r}

data("stop_words")
# head(stopwords()) #```{r} I corrected the head function in the following line. 
head(stop_words) 

tidy_trump_tweets <- 
  tidy_trump_tweets %>% 
  anti_join(stop_words)


head(tidy_trump_tweets, 20)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))
  
```

```{r}

#tidy_trump_tweets <-
#  tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ] 

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  filter(!grepl("https|t.co|amp|rt", word))

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))



# Enlever les chiffres

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  filter(!grepl("\\b\\d+\\b", word))


#tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]

# OUne fois de plus, tidytext rend automatiquement tous les mots en minuscules.

# Enlever l'espace

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  mutate(word = gsub("\\s+","", word))

tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)    

```

```{r}

tidy_trump_tweets %>% 
  count(word, sort = TRUE)


tidy_trump_tweets %>%          # Same as previously but with arrange
  count(word) %>% 
  arrange(desc(n))

# Sélectionner les 20 mots les plus importants

top_20 <- 
  tidy_trump_tweets %>% 
  count(word, sort = TRUE) 

top_20 <- top_20[1:20, ]    
  
top_20  



```

```{r}
library(ggplot2) # ```{r} I added the library function for ggplot.

ggplot(top_20) +
  geom_bar(aes(x = word, y = n, fill = word), stat = "identity") +
  theme_minimal() +
  theme(axis.text = element_text(angle = 90, hjust = 1)) +
  labs(x = "mot", y = "Nombre de fois que le mot apparait dans un tweet") +
  guides(fill = FALSE)

```

```{r}


tidy_trump_tfidf <- 
  trumptweets %>%
  select(created_at, text) %>%
  unnest_tokens("word", text) %>%
 # anti_join(stop_words) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)


top_tfidf <- tidy_trump_tfidf %>%
  arrange(desc(n))

top_tfidf

head(top_tfidf$word)

top_tfidf <- 
  tidy_trump_tfidf %>% 
  arrange(desc(tf_idf))

top_tfidf

```


```{r}

#economic_dictionary <- c("economy","unemployment","trade","tariffs")

economic_dictionary <- "economy|unemployment|trade|tariffs|employment"

economic_dictionary1 <- "Togo|Cameroon"


```


```{r}
library(tidyverse) #```{r} I added library function for tidyverse so that the pipe operator would function 
economic_tweets <- 
  trumptweets %>% 
  filter(str_detect(text, economic_dictionary))

head(economic_tweets$text)


```

```{r}
library(tidytext)
head(get_sentiments("afinn"), 24)

summary(get_sentiments("afinn"))

head(get_sentiments("bing"), 24)
summary(get_sentiments("bing"))

```

```{r}

dictionnaire <- get_sentiments("bing")

trump_tweet_sentiment <- 
  tidy_trump_tweets %>%
  inner_join(dictionnaire) %>%
  count(created_at, sentiment) 

               
head(trump_tweet_sentiment)

#head(tidy_trump_tweets)

ggplot(trump_tweet_sentiment, aes(x=created_at, y=n, color=sentiment))+
  geom_line(size=.5)+
  theme_minimal()+
  labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
  theme_bw()

```

```{r}

library(lubridate)

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  mutate(date = date(created_at))


# Utilisons un meilleur regroupement

trump_tweet_sentiment1 <- 
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, sentiment) 

ggplot(trump_tweet_sentiment1, aes(x=date, y=n, color=sentiment))+
  geom_line(size=.5)+
  theme_minimal()+
  labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
  theme_bw()

```

```{r}

trump_sentiment_negatif <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(sentiment=="negative") %>%
  count(date, sentiment)

trump_sentiment_negatif

trump_sentiment_positif <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(sentiment=="positive") %>%
  count(date, sentiment)

trump_sentiment_positif

trump_sentiment <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  count(date, sentiment)

trump_sentiment 

```

```{r}

negatif <-
  ggplot(trump_sentiment_negatif) +
  geom_line(aes(x = date, y = n), color = "red") +
  labs(x = "Date", y = "Fréquence de mots négative dans les tweet de Trump")

negatif

```

```{r}

trump_approval <- read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")

head(trump_approval)

trump_approval <-
  trump_approval %>% 
  mutate(date = mdy(modeldate))

head(trump_approval)


approval_plot <-
  trump_approval %>%
  filter(subgroup == "Adults") %>%
  #filter(date > min(trump_sentiment_plot$date)) %>% 
  group_by(date) %>%
  summarise(approval = mean(approve_estimate))

head(approval_plot)

# Graphique

approval <-
  ggplot(approval_plot)+
  geom_line(aes(x = date, y = approval))+
  #theme_minimal()+
  labs(x = "Date", y = "% des Américains qui aprouvent Trump")

approval
```

```{r}
library(ggpubr)

ggarrange(negatif, approval, nrow = 2)

```

```{r}

nrc <- get_sentiments("nrc")
nrc

trump_tweet_sentiment_nrc <-
  tidy_trump_tweets %>% 
  inner_join(nrc) %>% 
  count(date, sentiment)

trump_tweet_sentiment_nrc



# Roue des sentiments

tidy_trump_tweets_nrc1 <-
  tidy_trump_tweets %>%  
  inner_join(nrc) %>%
  group_by(sentiment) %>% 
  count() %>% 
  ungroup()%>% 
  arrange(desc(sentiment)) %>%
  mutate(percentage = round(n/sum(n),4)*100,
         lab.pos = cumsum(percentage)-.5*percentage)

ggplot(data = tidy_trump_tweets_nrc1, 
       aes(x = 2, y = percentage, fill = sentiment))+
  geom_bar(stat = "identity")+
  coord_polar("y", start = 200) +
  geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
  theme_void() +
  #scale_fill_brewer(palette = "Dark2")+
  xlim(.5, 2.5) +
  ggtitle("Sentiment dans les tweets de Trump")

```



Il existe de nombreux autres types d'analyse des sentiments, que nous n'avons pas le temps de couvrir ici. Cependant, vous devez savoir que différents outils d'analyse des sentiments fonctionnent mieux pour certains corpus que pour d'autres. Voici une figure d'un article récent qui applique une variété de dictionnaires de sentiments différents à différents corpus:

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/comparaison.png)
Source: http://homepages.dcc.ufmg.br/~fabricio/download/cosn127-goncalves.pdf



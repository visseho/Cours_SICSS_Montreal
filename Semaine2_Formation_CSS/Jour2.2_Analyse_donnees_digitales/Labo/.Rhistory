wordcloud(.$word, .$tf_idf, max.words = 20)
}
View(corpus_tidy_text_tfidf)
hm <- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
corpus_tidy_text_tfidf <-
corpus_tidy_text_tfidf %>%
mutate(Author = if_else(id_num %in% hm, "Hamilton", "NA"))
Hamilton <- corpus_tidy_text_tfidf %>%
filter(Author == "Hamilton")
View(Hamilton)
View(Hamilton)
Hamilton_dtm <- Hamilton %>%
cast_dtm(id_num, word, tf_idf)
inspect(Hamilton_dtm[1:5, 2:7])
inspect(Hamilton_dtm[7:10, 2:7])
CLUSTERS <- 4
kmean_out <-
Hamilton_dtm %>%
kmeans(centers = CLUSTERS, nstart = 10)
hamilton_words <-
tibble(word = colnames(Hamilton_dtm))
hamilton_words <- bind_cols(hamilton_words, as_tibble(t(kmean_out$centers)))
View(hamilton_words)
top_word <-
hamilton_words %>%
pivot_longer(cols = c("1":"4"), "cluster", "value")
View(top_word)
top_word <-
hamilton_words %>%
pivot_longer(cols = c("1":"4"), "cluster", "value") %>%
group_by(cluster) %>%
top_n(10, value)
ggplot(top_word %>% filter(cluster == 1)) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip()
ggplot(top_word %>% filter(cluster == 2)) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip()
facet_wrap(~cluster)
ggplot(top_word) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip()
ggplot(top_word) +
geom_col(aes(x = reorder(word, desc(value)), y = value)) +
coord_flip() +
facet_wrap(~cluster)
top_word_summary <-
top_word %>%
group_by(cluster) %>%
summarise(top_word = str_c(word, collapse = ", "))
top_word_summary
library(topicmodels)
View(Hamilton)
Hamilton_dtm_tf <- Hamilton %>%
cast_dtm(id_num, word, n)
inspect(Hamilton_dtm[1:5, 2:6])
topic_hamilton_lda <- LDA(Hamilton_dtm_tf, k = 4, control = list(seed = 2342))
topic_hamilton_data <- tidy(topic_hamilton_lda, matrix = "beta")
topic_hamilton_data
tt_top_term <-
topic_hamilton_data %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
View(tt_top_term)
View(tt_top_term)
tt_top_term %>%
filter(term != "corpus") %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
>>>>>>> 08b043a685b19bdde9cea0c71b5075ff89e47a07
library(tidyverse)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
trumptweets <- readRDS("../Données/trumptweets.Rdata")
View(trumptweets)
head(trumptweets)
trumptweets$text[2]
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
View((trump_corpus))
# Les informations de chaque document
trump_corpus[[1]][["content"]]
View(trump_corpus)
# Les informations de chaque document
trump_corpus[[1]][["content"]]
trump_corpus[[2]][["meta"]]
trump_corpus[[1]][["meta"]]
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
# Transformer en minuscule
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower))
# Enlever les espaces
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
# Stemming
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(3, Inf)))
inspect(trump_DTM[1:6,1:10])
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(5, Inf)))
inspect(trump_DTM[1:6,1:10])
trump_DTM_matrix <- as.matrix(trump_DTM)
trump_DTM_matrix[1:5,1:8]
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
View(tidy_trump_tweets)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
data("stop_words")
head(stopwords(), 24)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
http_trump <- data.frame(word = c("https", "t.co", "amp", "rt"))
http_trump
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(http_trump)
tidy_trump_tweets  %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets <- tidy_trump_tweets %>%
mutate_at("word", funs(wordStem((.), language="en")))
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
# Document-term matrix
tidy_trump_DTM <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(tidy_trump_DTM[1:5, 1:8])
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
View(top_20)
top_20 <- top_20[1:20, ]
top_20
View(top_20)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE)
tidy_trump_tweets_tfidf <-
tidy_trump_tweets %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
View(tidy_trump_tweets_tfidf)
top_tfidf <-
tidy_trump_tweets_tfidf %>%
arrange(desc(tf_idf))
View(top_tfidf)
tidy_trump_tfidf<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text) %>%
anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n) %>%
filter(tf_idf < 4)
tidy_trump_tfidf<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text) %>%
anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n) %>%
filter(tf_idf < 4)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
top_tfidf$word[1]
View(tidy_trump_tweets_tfidf)
rm(list = ls())
library(tidyverse)
library(tidytext)
library(textdata) #
#install.packages("tidytext")
install.packages("textdata")
library(textdata) #
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
trumptweets <- readRDS("../Données/trumptweets.RData")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("https|t.co|amp|rt", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tfidf <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text) %>%
# anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
View(tidy_trump_tweets)
economic_dictionary <- "economy|unemployment|trade|tariffs|employment|tax"
economic_dictionary1 <- "Togo|Cameroon"
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary))
View(economic_tweets)
head(get_sentiments("afinn"), 24)
summary(get_sentiments("afinn"))
head(get_sentiments("afinn"), 24)
head(get_sentiments("bing"), 24)
summary(get_sentiments("bing"))
head(get_sentiments("bing"), 24)
head(get_sentiments("nrc"), 24)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
dictionnaire <- get_sentiments("bing")
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
View(trump_tweet_sentiment)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire)
View(trumptweets)
View(trump_tweet_sentiment)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
View(trump_tweet_sentiment)
ggplot(trump_tweet_sentiment) +
geom_line(aes(x=created_at, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
View(tidy_trump_tweets)
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(date = date(created_at))
View(tidy_trump_tweets)
trump_tweet_sentiment1 <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
ggplot(trump_tweet_sentiment1) +
geom_line(aes(x=date, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
trump_sentiment_negatif <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="negative") %>%
count(date, sentiment)
trump_sentiment_positif <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="positive") %>%
count(date, sentiment)
trump_sentiment <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
negatif <-
ggplot(trump_sentiment_negatif) +
geom_line(aes(x = date, y = n), color = "red") +
labs(x = "Date", y = "Fréquence de mots négative dans les tweet de Trump")
negatif
trump_approval <- read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")
View(trump_approval)
trump_approval <-
trump_approval %>%
mutate(date = mdy(modeldate))
approval_plot <-
trump_approval %>%
filter(subgroup == "Adults") %>%
#filter(date > min(trump_sentiment_plot$date)) %>%
group_by(date) %>%
summarise(approval = mean(approve_estimate))
head(approval_plot)
approval <-
ggplot(approval_plot) +
geom_line(aes(x = date, y = approval)) +
#theme_minimal()+
labs(x = "Date", y = "% des Américains qui aprouvent Trump")
approval
ggarrange(negatif, approval, nrow = 2)
library(ggpubr)
ggarrange(negatif, approval, nrow = 2)
nrc <- get_sentiments("nrc")
trump_tweet_sentiment_nrc <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
count(date, sentiment)
trump_tweet_sentiment_nrc
tidy_trump_tweets_nrc1 <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
group_by(sentiment) %>%
count() %>%
ungroup()%>%
arrange(desc(sentiment)) %>%
mutate(percentage = round(n/sum(n), 4)*100,
lab.pos = cumsum(percentage)-.5*percentage)
ggplot(data = tidy_trump_tweets_nrc1, aes(x = 2, y = percentage, fill = sentiment))+
geom_bar(stat = "identity")+
coord_polar("y", start = 200) +
geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
theme_void() +
#scale_fill_brewer(palette = "Dark2")+
xlim(.5, 2.5) +
ggtitle("Sentiment dans les tweets de Trump")
library(tidyverse)
library(magrittr)
library(rvest)
webpage <- read_html("https://guides.loc.gov/federalist-papers/text-1-10")
page_section <- html_node(webpage, xpath = '//*[@id="s-lg-col-1"]/div')
file <- html_text(page_section)
data1 <- data_frame(text = file)
data1$text
data1 <-
data1 %>%
separate(text, into = c("text1", "text2", "text3", "text4", "text5",
"text6", "text7", "text8", "text9", "text10"), sep = "Back to text")
View(data1)
page_section1 <- html_node(webpage, css = ".s-lg-box-wrapper-25493264 .s-lib-box-std")
file1 <- html_text(page_section1)
file1
page <- c("1-10", "11-20", "21-30", "31-40", "41-50", "51-60", "61-70", "71-80", "81-85")
length(page)
for(i in seq_along(page)) {
data <- read_html(webpage) %>%
html_nodes("table") %>%
//*[@id="s-lg-box-wrapper-25493264"] %>%
for(i in 0:1) {
webpage <- read_html(paste0("https://bra.areacodebase.com/number_type/M?page=", i))
data <- webpage %>%
html_nodes("table") %>%
.[[1]] %>%
html_table()
}
for(i in 0:1) {
webpage <- read_html(paste0("https://bra.areacodebase.com/number_type/M?page=", i))
data <- webpage %>%
html_nodes("table") %>%
.[[1]] %>%
html_table()
}
webpage_section <- html_node(webpage, css = '.views-field')
webpage_section <- html_node(webpage, css = '.views-field')
web_table <- html_text(webpage_section)
web_table
library(qss)
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus_raw <- VCorpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus_raw
corpus_raw[[1]][1]
corpus_tidy <- tidy(corpus_raw, "corpus")
View(corpus_tidy)
library(tidyverse)
library(tidytext)
library(tidytext)
library(textdata)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
library(topicmodels)
library(topicmodels)
trumptweets <- readRDS("../Données/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ]
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
trump_tweets_dtm <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(trump_tweets_dtm[1:5,1:6])
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 5, control = list(seed = 123))
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_topics
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta)
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 3, control = list(seed = 123))
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_topics
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
webpage <- read_html("https://guides.loc.gov/federalist-papers/text-1-10")
rm(list = ls())
library(tidyverse)
library(magrittr)
library(rvest)
webpage <- read_html("https://guides.loc.gov/federalist-papers/text-1-10")
webpage <- read_html("https://guides.loc.gov/federalist-papers/text-1-10")
section <- html_nodes(webpage, css = ".s-lib-box-std")
file <- html_table(section)
View(file)
View(section)
section[[1]][1]
section[[1]][[1]]
section[[1]][[2]]
section[[1]][[3]]
section[[1]][[1]]
section[[1]][[1]][1]
section[[1]][[2]][1]
section[[1]][[2]]
View(section)
file <- html_text(section)
file
text <- "Row 1, Cell 1\tnRow 1, Cell 2\tnRow 1, Cell 3\tnRow 1, Cell 4\nRow 2, Cell 1\tnRow 2, Cell 2\tnRow 2, Cell 3\tnRow 2, Cell 4"
text
# Step 1: Split the text into rows
rows <- strsplit(text, "\n")[[1]]
rows
# Step 2: Split each row into cells
table <- lapply(rows, function(row) strsplit(row, "\t")[[1]])
# Step 3: Create a data frame from the list
df <- data.frame(matrix(unlist(table), nrow = length(table), byrow = TRUE))
View(df)
rows <- strsplit(file, "\n")[[1]]
rows
table <- lapply(rows, function(row) strsplit(row, "\t")[[1]])
df <- data.frame(matrix(unlist(table), nrow = length(table), byrow = TRUE))

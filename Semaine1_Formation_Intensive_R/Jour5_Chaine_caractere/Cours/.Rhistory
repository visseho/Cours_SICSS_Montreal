freq(cis_2015$PROV)
freq(cis_2015$AGEGP)
View(cis_2015)
cis_short <-
cis_2015 %>%
select(PUMFID, PERSONID, AGEGP, SEX, MARST, PROV, IMMSTP, YRIMMGP, CMPHI, HLEV2G, CFID, CFSIZE, CFMJSI, CFATINC, CFEARNG, EFID, EFSIZE, EFATINC, HHSIZE, FWEIGHT) %>%
mutate(Province = case_when(
PROV == 10 ~ "Terre-Neuve-et-Labrador",
PROV == 11 ~ "Ile du prince Edouard",
PROV == 12 ~ "Nouvelle-Écosse",
PROV == 13 ~ "Nouveau-Brunswick",
PROV == 24 ~ "Québec",
PROV == 35 ~ "Ontario",
PROV == 46 ~ "Manitoba",
PROV == 47 ~ "Saskatchewan",
PROV == 48 ~ "Alberta",
PROV == 59 ~ "Colombie Britanique"
),
region = case_when(
PROV <= 13 ~ "Atlantique",
PROV == 24 ~ "Québec",
PROV == 35 ~ "Ontario",
PROV >=46 & PROV <= 48 ~ "Ouest",
PROV == 59 ~ "Colombie Britanique"
),
sexe = if_else(SEX == 1, "Male", "Female"))
View(cis_short)
mean_income <-
cis_short %>%
group_by(Province) %>%
summarise(mean(CFATINC))
mean_income
mean_income <-
cis_short %>%
group_by(Province) %>%
summarise(revenu_moyen = mean(CFATINC))
mean_income
mean_income1 <-
cis_short %>%
filter(CFATINC >= 0) %>%
group_by(Province) %>%
summarise(mean(CFATINC))
mean_income1
income_mean <- bind_cols(mean_income, mean_income1)
income_mean
variation_income <-
cis_short %>%
group_by(Province) %>%
summarise(variance_revenu = var(CFATINC, na.rm = TRUE))
variation_income
variation_income <-
cis_short %>%
group_by(Province) %>%
summarise(variance_revenu = var(CFATINC, na.rm = TRUE),
écart-type = sd(CFATINC, na.rm = TRUE))
variation_income <-
cis_short %>%
group_by(Province) %>%
summarise(variance_revenu = var(CFATINC, na.rm = TRUE),
écart_type = sd(CFATINC, na.rm = TRUE))
variation_income
ggplot(cis_short) +
geom_histogram(aes(x = CFATINC))
ggplot(cis_short) +
geom_boxplot(aes(y = CFATINC))
ggplot(cis_short) +
geom_boxplot(aes(y = CFATINC)) +
facet_grid(~ Province)
ggplot(cis_short) +
geom_boxplot(aes(x = Province, y = CFATINC, color = Province))
ggplot(cis_short) +
geom_boxplot(aes(x = Province, y = CFATINC), color = "green")
ggplot(cis_short) +
geom_boxplot(aes(x = Province, y = CFATINC), color = "lighblue")
ggplot(cis_short) +
geom_boxplot(aes(x = Province, y = CFATINC), color = "lightblue")
ggplot(cis_short) +
geom_boxplot(aes(x = Province, y = CFATINC), color = "blue")
outliers <-
cis_short %>%
summarise(Income_Q1 = quantile(CFATINC, prob = 0.25),
Income_Q3 = quantile(CFATINC, prob = 0.75),
ecart_IQ = Income_Q3 - Income_Q1,
valeur_haute = Income_Q3 + 1.5*ecart_IQ,
valeur_basse = Income_Q1 - 1.5*ecart_IQ)
outliers
View(outliers)
cis_no_outliers <-
cis_short %>%
filter(CFATINC <= outliers$valeur_haute & CFATINC >= outliers$valeur_basse)
ggplot(cis_no_outliers) +
geom_boxplot(aes(y = CFATINC))
ggplot(cis_no_outliers) +
geom_boxplot(aes(y = CFATINC)) +
facet_grid(~Province)
ggplot(cis_short %>% filter(Province == "Quebec")) +
geom_boxplot(aes(y = CFATINC))
ggplot(cis_short %>% filter(Province == "Québec")) +
geom_boxplot(aes(y = CFATINC))
rm(list = ls())
library(tidyverse)
library(summarytools)
restaurants <- read_csv("../Donnees/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
View(restaurants)
glimpse(restaurants)
restaurants <-
restaurants %>%
mutate(ZIP_length = nchar(Zip_Code)) #%>%
freq(restaurants$ZIP_length)
restaurants <-
restaurants %>%
mutate(ZIP_5 = substr(Zip_Code, 1, 5)
restaurants <-
restaurants <-
restaurants %>%
mutate(ZIP_5 = substr(Zip_Code, 1, 5),
ZIP_3 = substr(Zip_Code, 2, 4))
freq(restaurants$ZIP_5)
restaurants <-
restaurants %>%
mutate(ZIP_5 = substr(Zip_Code, 1, 5),
ZIP_3 = substr(Zip_Code, 2, 4),
count_zip5 = nchar(ZIP5))
restaurants <-
restaurants %>%
mutate(ZIP_5 = substr(Zip_Code, 1, 5),
ZIP_3 = substr(Zip_Code, 2, 4),
count_zip5 = nchar(ZIP_5))
freq(restaurants$count_zip5)
restaurants <-
restaurants %>%
mutate(mailing_address =
paste(Address, ", ", City, ", WA ", ZIP_5, sep = ""))
restaurants <-
restaurants %>%
mutate(mailing_address =
paste(Address, ", ", City, ", WA ", ZIP_5, sep = "+"))
1:5
letters[1:5]
paste(1:5, letters[1:5]) # sep met un espace par défaut
paste0(1:5, letters[1:5])
paste(letters[1:5], sep = "+")
paste(1:5, letters[1:5], sep = "+")
letters[1:5]
paste(letters[1:5], collapse = "!")
paste0(1:5, letters[1:5], collapse = "???")
paste(1:5, "Z", sep = "*")
paste(1:5, "Z", collapse = "*")
paste(1:5, letters[1:5]) # sep met un espace par défaut
paste(1:5, "Z", collapse = "*")
paste(1:5, "Z", collapse = "*", sep = "")
paste(1:5, "Z", sep = "*", collapse = " ~ ")
str_sub("Washington", 3, 8)
str_sub("Washington", 8, 3) # Pourquoi ceci ne marche pas?
str_sub("Washington", -5, -3)
str_sub("Washington", -3, -5) # Pourquoi ceci ne marche pas?
letters
letters[1:5]
1:5
str_c(letters[1:5], 1:5)
?str_c
str_c(letters[1:5], 1:5)
str_c(letters[1:5], 1:5, sep = "e")
str_c(letters[1:5], 1:5, sep = "*")
str_c(letters[1:5], 1:5, collapse = "*")
str_length("voisines")
head(restaurants$City)
head(unique(restaurants$City))
head(unique(restaurants$Name))
head(unique(restaurants$Address))
restaurants <-
restaurants %>%
mutate(Name = str_to_upper(Name),
Address = str_to_upper(Address),
City = str_to_upper(City))
head(unique(restaurants$City))
head(unique(restaurants$Name), 20)
restaurants <-
restaurants %>%
mutate_if(is.character, str_trim)
coffee <-
restaurants %>%
filter(str_detect(Name, "COFFEE|ESPRESSO|ROASTER|CAFE"))
View(coffee)
head(unique(restaurants$Phone))
exemple_test <- c("2061234567", "(206)1234567",
"(306) 123-4567", "555-206-1234")
exemple_test
str_detect(exemple_test, 206)
str_detect(exemple_test, "206")
str_detect(exemple_test, "^\\(?206")
code_206_pattern <- "^\\(?206|^\\(?306"
str_detect(exemple_test, code_206_pattern)
code_206_pattern <- "^\\(206|^\\(306"
code_206_pattern <- "^\\(206|^\\(306"
str_detect(exemple_test, code_206_pattern)
code_206_pattern <- "^\\(?206|^\\(?306"
str_view(exemple_test, code_206_pattern)
restaurants %>%
mutate(contient_nombre_206 = str_detect(Phone, code_206_pattern)) %>%
group_by(contient_nombre_206) %>%
tally()
?tally
restaurants %>%
mutate(contient_nombre_206 = str_detect(Phone, code_206_pattern)) %>%
group_by(contient_nombre_206) %>%
tally()
direction_exemples <- c("2812 THORNDYKE AVE W", "512 NW 65TH ST",
"407 CEDAR ST", "15 NICKERSON ST ")
str_extract(direction_exemples, "N")
str_view(tr_extract(direction_exemples, "N"))
str_view(str_extract(direction_exemples, "N"))
str_view(str_extract(direction_exemples, " N"))
str_view(str_extract(direction_exemples, " N "))
direction_pattern <- " (N|NW|NE|S|SW|SE|W|E)( |$)"
direction_pattern1 <- "(N|NW|NE|S|SW|SE|W|E)"
str_extract(direction_exemples, direction_pattern1)
direction_pattern <- " (N|NW|NE|S|SW|SE|W|E)( |$)"
direction_pattern1 <- "(N|NW|NE|S|SW|SE|W|E)"
direction_exemples <- c("2812 THORNDYKE AVE W", "512 NW 65TH ST",
"407 CEDAR ST", "15 NICKERSON ST ")
str_extract(direction_exemples, direction_pattern1)
str_extract(direction_exemples, direction_pattern)
restaurants %>%
distinct(Address) %>%
mutate(city_region = #str_extract(Address, direction_pattern)) %>%
str_trim(str_extract(Address, direction_pattern)))
#restaurants <-
restaurants %>%
distinct(Address) %>%
mutate(city_region = #str_extract(Address, direction_pattern)) %>%
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
address_number_test_examples <-
c("2812 THORNDYKE AVE W", "1ST AVE", "10AA 1ST AVE",
"10-A 1ST AVE", "5201-B UNIVERSITY WAY NE",
"7040 1/2 15TH AVE NW")
address_number_pattern <- "^[0-9]*-?[A-Z]? (1/2 )?"
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]+? (1/2 )?"
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]? (1/2 )?"
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]*? (1/2 )?"
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]? (1/2 )?"
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]*? (1/2 )?"
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
restaurants <-
restaurants %>%
mutate(street_only = str_replace(Address, address_number_pattern,
replacement = ""))
restaurants %>% distinct(street_only) %>% head(11)
address_unit_test_examples <-
c("1ST AVE", "RAINIER AVE S #A", "FAUNTLEROY WAY SW STE 108",
"4TH AVE #100C", "NW 54TH ST")
address_unit_test_examples <-
c("1ST AVE", "RAINIER AVE S #A", "FAUNTLEROY WAY SW STE 108",
"4TH AVE #100C", "NW 54TH ST")
str_replace(address_unit_test_examples, address_unit_pattern,
replacement = "")
address_unit_pattern <- " (#|STE|SUITE|SHOP|UNIT).*$"
str_replace(address_unit_test_examples, address_unit_pattern,
replacement = "")
address_unit_pattern <- " (#|STE|SUITE|SHOP|UNIT).*$"
restaurants <-
restaurants %>%
mutate(street_only = str_trim(str_replace(street_only,
address_unit_pattern, replacement = "")))
restaurants %>%
distinct(Business_ID, Date, Inspection_Score, street_only) %>%
filter(Inspection_Score > 45) %>%
count(street_only) %>%
arrange(desc(n)) %>%
head(n=5)
head(unique(restaurants$Violation_Description))
head(str_split_fixed(restaurants$Violation_Description, " - ", n = 2))
head(unique(restaurants$Violation_Description))
head(str_split_fixed(restaurants$Violation_Description, "-", n = 2))
head(unique(restaurants$Violation_Description))
head(str_split_fixed(restaurants$Violation_Description, " - ", n = 2))
knitr::include_graphics("../Images/science_policy.jpg")
knitr::include_graphics("../Images/science_policy.jpg")
knitr::include_graphics("../Images/chiffre_internet1.jpeg")
knitr::include_graphics("../Images/chiffre_internet1.jpeg")
knitr::include_graphics("../Images/chiffre_internet1.jpg")
knitr::include_graphics("../Images/c10continue.png")
knitr::include_graphics("../Images/c10continue2.png")
knitr::include_graphics("../Images/c10reactive.png")
knitr::include_graphics("../Images/c10relation.png")
knitr::include_graphics("../Images/c10reactive.png")
rm(list = ls())         # Clear your environment
getwd()                 # Tell you your working directory
library(tidyverse)      # The most important packages
library(rio)            # To read Stata, SPSS and other files in R
library(ggpubr)         # help to combine figure
library(haven)          # To read Stata, SPSS and other files in R
library(broom)
library(readstata13)
library(stargazer)      # Produce regression table as in articles
library(summarytools)   # For descriptive analysis
library(scales)
library(srvyr)
library(survey)
library(plotrix)
library(foreign)        # To read Stata, SPSS and other files in R
datasets <- dhs_datasets(
countryIds = c("ZW"),
selectSurveys = T,
surveyIds = NULL,
surveyYear = NULL,
surveyYearStart = NULL,
surveyYearEnd = NULL,
surveyType = "DHS",
fileFormat = "stata",
fileType = c("HR", "IR","PR"),
f = NULL,
returnFields = NULL,
perPage = NULL,
page = NULL,
client = NULL,
force = FALSE,
all_results = TRUE
) %>%
select(CountryName, SurveyYear, SurveyId, FileType, FileName) %>%
group_by(CountryName) %>%
filter(SurveyYear == max(SurveyYear)) %>%
ungroup()
library(rdhs)
library(rdhs)
datasets <- dhs_datasets(
countryIds = c("ZW"),
selectSurveys = T,
surveyIds = NULL,
surveyYear = NULL,
surveyYearStart = NULL,
surveyYearEnd = NULL,
surveyType = "DHS",
fileFormat = "stata",
fileType = c("HR", "IR","PR"),
f = NULL,
returnFields = NULL,
perPage = NULL,
page = NULL,
client = NULL,
force = FALSE,
all_results = TRUE
) %>%
select(CountryName, SurveyYear, SurveyId, FileType, FileName) %>%
group_by(CountryName) %>%
filter(SurveyYear == max(SurveyYear)) %>%
ungroup()
View(datasets)
rm(list = ls())
library(tidyverse)
library(rvest)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
View(wikipedia_page)
wikipedia_page
Section_wikipedia <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')
health_rankings <- html_table(Section_wikipedia)
View(health_rankings)
section_wiki_f <- html_node(wikipedia_page, xpath = '/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table')
View(health_rankings)
ggplot(health_rankings) +
geom_histogram(aes(x = `Attainment of goals / Health / Level (DALE)`))
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia1 <- html_node(wikipedia_page, css = '#mw-content-text > div.mw-parser-output > table')
section_wiki <- html_node(wikipedia_page, css = '.jquery-tablesorter td , .headerSort')
section_wiki <- html_nodes(wikipedia_page, css = '.jquery-tablesorter td , .headerSort')
section_wiki <- html_nodes(wikipedia_page, css = '//*[@id="mw-content-text"]/div[1]/table')
section_wiki <- html_node(wikipedia_page, css = 'a .thumbborder , .flagicon+ a , .jquery-tablesorter tr+ tr td , td+ td , .headerSort')
table_wiki1 <- html_table(Section_wikipedia1)
table1_wiki <- html_table(section_wiki)
View(table1_wiki)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
section_essai <- html_node(wikipedia_page, css = '.headerSort')
section_essai <- html_node(wikipedia_page, css = 'td , .headerSort')
table <- html_table(section_essai)
table <- html_text(section_essai)
table
section_essai <- html_node(wikipedia_page, css = 'a .thumbborder , td+ td , .headerSort')
table <- html_text(section_essai)
table
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia1 <- html_node(wikipedia_page, css = '#mw-content-text > div.mw-parser-output > table')
table_wiki1 <- html_table(Section_wikipedia1)
View(table_wiki1)
lego <- read_html("https://www.imdb.com/title/tt1490017/") # ce chunk ne marche pas et je n'arrive pas à l'arranger (trouvé le xpath)
section_acteur <- html_node(lego, xpath = '//*[@id="__next"]/main/div/section[1]/div/section/div/div[1]/section[4]/div[2]/div[2]/div[1]/div[2]/a')
acteur_class <- html_text(section_acteur)
acteurs <- html_nodes(lego, xpath = '.primary_photo+ td a')
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td a')
acteurs
acteurs
# Collecter les noms
acteurs_nom <- html_text(acteurs, trim = TRUE)
acteurs_nom
rm(list = ls())
#install.packages("rtweet")
library(rtweet)
library(httpuv)
library(maps)
consumer_secret <- "xhYID0ORAUDNzpJC0Y1mupB7Hpv6dkGvpSqFFdMHpqy0pd1ZOi"
app_name <- "cssforafrica"
consumer_key <- "piMILL2EfDVHIDlwJlizUt6CY"
consumer_secret <- "xhYID0ORAUDNzpJC0Y1mupB7Hpv6dkGvpSqFFdMHpqy0pd1ZOi"
google_key <- "AIzaSyCthR_V7YJNduHuA7jrLunlQQxukFYyhs4"
create_token(app = app_name, consumer_key = consumer_key, consumer_secret = consumer_secret, set_renv = TRUE)
create_token(app = app_name, consumer_key = consumer_key, consumer_secret = consumer_secret, set_renv = TRUE)
create_token(app = app_name, consumer_key = consumer_key, consumer_secret = consumer_secret, set_renv = TRUE)
#DHS credentials:
set_rdhs_config(email = "visseho09@yahoo.fr",
project = "Reproductive health behavior and ch...",
config_path = "~/.rdhs.json",
global = TRUE,
verbose_download = TRUE)
datasets <- dhs_datasets(
countryIds = c("ZW"),
selectSurveys = T,
surveyIds = NULL,
surveyYear = NULL,
surveyYearStart = NULL,
surveyYearEnd = NULL,
surveyType = "DHS",
fileFormat = "stata",
fileType = c("HR", "IR","PR"),
f = NULL,
returnFields = NULL,
perPage = NULL,
page = NULL,
client = NULL,
force = FALSE,
all_results = TRUE
) %>%
select(CountryName, SurveyYear, SurveyId, FileType, FileName) %>%
group_by(CountryName) %>%
filter(SurveyYear == max(SurveyYear)) %>%
ungroup()
#Getting datasets
downloads <- get_datasets(datasets$FileName)
View(datasets)
View(downloads)
trumptweets <- readRDS("../Données/trumptweets.Rdata")
trumptweets <- readRDS("../Données/trumptweets.Rdata")
academictwitteR::set_bearer()
install.packages("academictwitteR")
library(academictwitteR)
academictwitteR::set_bearer()
View(trumptweets)
rm(list = ls())         # Clear your environment
getwd()                 # Tell you your working directory
library(tidyverse)      # The most important packages
library(rio)            # To read Stata, SPSS and other files in R
library(ggpubr)         # help to combine figure
library(haven)          # To read Stata, SPSS and other files in R
library(broom)
library(readstata13)
library(stargazer)      # Produce regression table as in articles
library(summarytools)   # For descriptive analysis
library(scales)
library(srvyr)
library(survey)
library(plotrix)
library(foreign)        # To read Stata, SPSS and other files in R
library(rdhs)
datasets <- dhs_datasets(
countryIds = c("ZW"),
selectSurveys = T,
surveyIds = NULL,
surveyYear = NULL,
surveyYearStart = NULL,
surveyYearEnd = NULL,
surveyType = "DHS",
fileFormat = "stata",
fileType = c("HR", "IR","PR"),
f = NULL,
returnFields = NULL,
perPage = NULL,
page = NULL,
client = NULL,
force = FALSE,
all_results = TRUE
) %>%
select(CountryName, SurveyYear, SurveyId, FileType, FileName) %>%
group_by(CountryName) %>%
filter(SurveyYear == max(SurveyYear)) %>%
ungroup()
View(datasets)
#DHS credentials:
set_rdhs_config(email = "visseho09@yahoo.fr",
project = "Reproductive health behavior and ch...",
config_path = "~/.rdhs.json",
global = TRUE,
verbose_download = TRUE)
#Getting datasets
downloads <- get_datasets(datasets$FileName)
zwir <- readRDS(downloads$ZWIR72DT)
View(zwir)
#install.packages("academictwitteR")
library(academictwitteR)
academictwitteR::set_bearer()
